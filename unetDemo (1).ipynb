{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install torchsummary\n","!pip install torchgeometry"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from torchsummary import summary\n","from torchgeometry.losses import one_hot\n","import os\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch import Tensor\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\n","from collections import OrderedDict\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of class in the data set (3: neoplastic, non neoplastic, background)\n","num_classes = 3\n","\n","# Number of epoch\n","epochs = 15\n","\n","# Hyperparameters for training \n","learning_rate = 2e-04\n","batch_size = 4\n","display_step = 50\n","\n","# Model path\n","checkpoint_path = '/kaggle/working/unet_model.pth'\n","pretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n","# Initialize lists to keep track of loss and accuracy\n","loss_epoch_array = []\n","train_accuracy = []\n","test_accuracy = []\n","valid_accuracy = []"]},{"cell_type":"markdown","metadata":{},"source":["# Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ToTensor\n","\n","transform = Compose([\n","    Resize((256, 256) , interpolation=InterpolationMode.BILINEAR),\n","    RandomCrop(256),\n","    RandomHorizontalFlip(),\n","    RandomVerticalFlip(),\n","    ToTensor(),\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class UNetDataClass(Dataset):\n","    def __init__(self, images_path, masks_path, transform):\n","        super(UNetDataClass, self).__init__()\n","        \n","        images_list = os.listdir(images_path)\n","        masks_list = os.listdir(masks_path)\n","        \n","        images_list = [images_path + image_name for image_name in images_list]\n","        masks_list = [masks_path + mask_name for mask_name in masks_list]\n","        \n","        self.images_list = images_list\n","        self.masks_list = masks_list\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        img_path = self.images_list[index]\n","        mask_path = self.masks_list[index]\n","        \n","        # Open image and mask\n","        data = Image.open(img_path)\n","        label = Image.open(mask_path)\n","        \n","        # Normalize\n","        data = self.transform(data) / 255\n","        label = self.transform(label) / 255\n","        \n","        label = torch.where(label>0.65, 1.0, 0.0)\n","        \n","        label[2, :, :] = 0.0001\n","        label = torch.argmax(label, 0).type(torch.int64)\n","        \n","        return data, label\n","    \n","    def __len__(self):\n","        return len(self.images_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n","masks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unet_dataset = UNetDataClass(images_path, masks_path, transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_size = 0.8\n","valid_size = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_set, valid_set = random_split(unet_dataset, \n","                                    [int(train_size * len(unet_dataset)) , \n","                                     int(valid_size * len(unet_dataset))])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{},"source":["**Encoder Block**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.relu = nn.ReLU()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(),\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.shortcut = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","\n","    def forward(self, inputs):\n","        x1 = self.conv(inputs)\n","        x2 = self.shortcut(inputs)\n","        x = self.relu(x1 + x2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.r1 = ResidualBlock(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, inputs):\n","        x = self.r1(inputs)\n","        p = self.pool(x)\n","        return x, p"]},{"cell_type":"markdown","metadata":{},"source":["**Decoder block**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.relu = nn.ReLU()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(),\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.shortcut = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","\n","    def forward(self, inputs):\n","        x1 = self.conv(inputs)\n","        x2 = self.shortcut(inputs)\n","        x = self.relu(x1 + x2)\n","        return x\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.r1 = ResidualBlock(in_c[0]+in_c[1], out_c)\n","        self.r2 = ResidualBlock(out_c, out_c)\n","\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.r1(x)\n","        x = self.r2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["**Dilate**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DilatedConv(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.c1 = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, dilation=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","        self.c2 = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=3, dilation=3),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","        self.c3 = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=6, dilation=6),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","        self.c4 = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=9, dilation=9),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","        self.c5 = nn.Sequential(\n","            nn.Conv2d(out_c*4, out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, inputs):\n","        x1 = self.c1(inputs)\n","        x2 = self.c2(inputs)\n","        x3 = self.c3(inputs)\n","        x4 = self.c4(inputs)\n","        x = torch.cat([x1, x2, x3, x4], axis=1)\n","        x = self.c5(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["**Bottle neck**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class bottleneck_block(nn.Module):\n","    def __init__(self, in_c, out_c, dim, num_layers=2):\n","        super().__init__()\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=8)\n","        self.tblock = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        b, c, h, w = x.shape\n","        x = x.reshape((b, c, h*w))\n","        x = self.tblock(x)\n","        x = x.reshape((b, c, h, w))\n","        x = self.conv2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["**Unet model**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#import resnet50 \n","from torchvision.models import resnet50\n","\n","# ResnetUnet\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_class=3):\n","\n","        super(UNet, self).__init__()\n","        self.enc1 = encoder_block(3, 64)\n","        self.enc2 = encoder_block(64, 128)\n","        self.enc3 = encoder_block(128, 256)\n","        self.enc4 = encoder_block(256, 512)\n","        # backbone = resnet50(pretrained=True)\n","        # self.enc1 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n","        # self.enc2 = nn.Sequential(backbone.maxpool, backbone.layer1)\n","        # self.enc3 = backbone.layer2\n","        # self.enc4 = backbone.layer3\n","        \n","        # Bottleneck block\n","        self.bottleneck = bottleneck_block(512, 1024, 1024, num_layers=2)\n","        self.dilated_conv = DilatedConv(1024, 512)\n","        \n","        # Decoder blocks\n","        self.dec1 = DecoderBlock([512, 512], 256)\n","        self.dec2 = DecoderBlock([256, 256], 128)  \n","        self.dec3 = DecoderBlock([128, 128], 64)\n","        self.dec4 = DecoderBlock([64, 64], 32)\n","        \n","        # 1x1 convolution\n","        self.out = nn.Conv2d(32, n_class, kernel_size=1, padding='same')\n","        \n","    def forward(self, image):\n","        # Encoder\n","        enc1, pool1 = self.enc1(image)\n","        enc2, pool2 = self.enc2(pool1)\n","        enc3, pool3 = self.enc3(pool2)\n","        enc4, pool4 = self.enc4(pool3)\n","    \n","        # Bottleneck\n","        bottleneck = self.bottleneck(pool4)\n","        dilated_conv = self.dilated_conv(bottleneck)\n","        b3 = torch.cat([dilated_conv, bottleneck], axis=1)\n","        # Decoder\n","        dec1 = self.dec1(b3, enc4)\n","        dec2 = self.dec2(dec1, enc3)\n","        dec3 = self.dec3(dec2, enc2)\n","        dec4 = self.dec4(dec3, enc1)\n","\n","        # 1x1 convolution\n","        out = self.out(dec4)\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["# Loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CEDiceLoss(nn.Module):\n","    def __init__(self, weights) -> None:\n","        super(CEDiceLoss, self).__init__()\n","        self.eps: float = 1e-6\n","        self.weights: torch.Tensor = weights\n","\n","    def forward(\n","            self,\n","            input: torch.Tensor,\n","            target: torch.Tensor) -> torch.Tensor:\n","        if not torch.is_tensor(input):\n","            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n","                            .format(type(input)))\n","        if not len(input.shape) == 4:\n","            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n","                             .format(input.shape))\n","        if not input.shape[-2:] == target.shape[-2:]:\n","            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n","                             .format(input.shape, input.shape))\n","        if not input.device == target.device:\n","            raise ValueError(\n","                \"input and target must be in the same device. Got: {}\" .format(\n","                    input.device, target.device))\n","        if not self.weights.shape[1] == input.shape[1]:\n","            raise ValueError(\"The number of weights must equal the number of classes\")\n","        if not torch.sum(self.weights).item() == 1:\n","            raise ValueError(\"The sum of all weights must equal 1\")\n","            \n","        # cross entropy loss\n","        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n","        \n","        # compute softmax over the classes axis\n","        input_soft = F.softmax(input, dim=1)\n","\n","        # create the labels one hot tensor\n","        target_one_hot = one_hot(target, num_classes=input.shape[1],\n","                                 device=input.device, dtype=input.dtype)\n","\n","        # compute the actual dice score\n","        dims = (2, 3)\n","        intersection = torch.sum(input_soft * target_one_hot, dims)\n","        cardinality = torch.sum(input_soft + target_one_hot, dims)\n","\n","        dice_score = 2. * intersection / (cardinality + self.eps)\n","        \n","        dice_score = torch.sum(dice_score * self.weights, dim=1)\n","        \n","        return torch.mean(1. - dice_score) + celoss"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["**Initialize weights**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def weights_init(model):\n","    if isinstance(model, nn.Linear):\n","        # Xavier Distribution\n","        torch.nn.init.xavier_uniform_(model.weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_model(model, optimizer, path):\n","    checkpoint = {\n","        \"model\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, path)\n","\n","def load_model(model, optimizer, path):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint[\"model\"])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer"]},{"cell_type":"markdown","metadata":{},"source":["**Train model**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train function for each epoch\n","def train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n","    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n","    start_time = time.time()\n","    train_loss_epoch = 0\n","    test_loss_epoch = 0\n","    last_loss = 999999999\n","    model.train()\n","    for i, (data,targets) in enumerate(train_dataloader):\n","        \n","        # Load data into GPU\n","        data, targets = data.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","\n","        # Backpropagation, compute gradients\n","        loss = loss_function(outputs, targets.long())\n","        loss.backward()\n","\n","        # Apply gradients\n","        optimizer.step()\n","        \n","        # Save loss\n","        train_loss_epoch += loss.item()\n","        if (i+1) % display_step == 0:\n","#             accuracy = float(test(test_loader))\n","            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n","                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset), \n","                loss.item()))\n","                  \n","    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n","    train_loss_epoch/= (i + 1)\n","    \n","    # Evaluate the validation set\n","    model.eval()\n","    with torch.no_grad():\n","        for data, target in valid_dataloader:\n","            data, target = data.to(device), target.to(device)\n","            test_output = model(data)\n","            test_loss = loss_function(test_output, target)\n","            test_loss_epoch += test_loss.item()\n","            \n","    test_loss_epoch/= (i+1)\n","    \n","    return train_loss_epoch , test_loss_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test function\n","def test(dataloader):\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for i, (data, targets) in enumerate(dataloader):\n","            data, targets = data.to(device), targets.to(device)\n","            outputs = model(data)\n","            _, pred = torch.max(outputs, 1)\n","            test_loss += targets.size(0)\n","            correct += torch.sum(pred == targets).item()\n","    return 100.0 * correct / test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = UNet()\n","# model.apply(weights_init)\n","# model = nn.DataParallel(model)\n","# checkpoint = torch.load(pretrained_path)\n","\n","# new_state_dict = OrderedDict()\n","# for k, v in checkpoint['model'].items():\n","#     name = k[7:] # remove `module.`\n","#     new_state_dict[name] = v\n","# # load params\n","# model.load_state_dict(new_state_dict)\n","model = nn.DataParallel(model)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\n","loss_function = CEDiceLoss(weights)\n","\n","# Define the optimizer (Adam optimizer)\n","optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","# Learning rate scheduler\n","learing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_model(model, optimizer, checkpoint_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:49:19.946010Z","iopub.status.busy":"2023-11-08T07:49:19.945624Z"},"trusted":true},"outputs":[],"source":["wandb.login(\n","    # set the wandb project where this run will be logged\n","#     project= \"PolypSegment\", \n","    key = \"394e21a63511f54c0156f130523a5c9847a3c415\",\n",")\n","wandb.init(\n","    project = \"PolypSegment\"\n",")\n","# Training loop\n","train_loss_array = []\n","test_loss_array = []\n","last_loss = 9999999999999\n","for epoch in range(epochs):\n","    train_loss_epoch = 0\n","    test_loss_epoch = 0\n","    (train_loss_epoch, test_loss_epoch) = train(train_dataloader, \n","                                              valid_dataloader, \n","                                              learing_rate_scheduler, epoch, display_step)\n","    \n","    if test_loss_epoch < last_loss:\n","        save_model(model, optimizer, checkpoint_path)\n","        last_loss = test_loss_epoch\n","        \n","    learing_rate_scheduler.step()\n","    train_loss_array.append(train_loss_epoch)\n","    test_loss_array.append(test_loss_epoch)\n","    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n","#     train_accuracy.append(test(train_loader))\n","#     valid_accuracy.append(test(test_loader))\n","#     print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n","#                                         train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:13:45.693672Z","iopub.status.busy":"2023-11-08T07:13:45.693265Z","iopub.status.idle":"2023-11-08T07:13:45.698302Z","shell.execute_reply":"2023-11-08T07:13:45.697285Z","shell.execute_reply.started":"2023-11-08T07:13:45.693640Z"},"trusted":true},"outputs":[],"source":["# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:49:11.294533Z","iopub.status.busy":"2023-11-08T07:49:11.294153Z","iopub.status.idle":"2023-11-08T07:49:11.825883Z","shell.execute_reply":"2023-11-08T07:49:11.824679Z","shell.execute_reply.started":"2023-11-08T07:49:11.294502Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Plot the learning cure"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T22:06:32.721256Z","iopub.status.busy":"2023-11-07T22:06:32.720932Z","iopub.status.idle":"2023-11-07T22:06:32.977288Z","shell.execute_reply":"2023-11-07T22:06:32.976365Z","shell.execute_reply.started":"2023-11-07T22:06:32.721231Z"},"trusted":true},"outputs":[],"source":["# load_model(model, checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T21:49:10.962473Z","iopub.status.busy":"2023-11-07T21:49:10.962085Z","iopub.status.idle":"2023-11-07T21:49:10.967361Z","shell.execute_reply":"2023-11-07T21:49:10.966430Z","shell.execute_reply.started":"2023-11-07T21:49:10.962443Z"},"trusted":true},"outputs":[],"source":["plt.rcParams['figure.dpi'] = 90\n","plt.rcParams['figure.figsize'] = (6, 4)\n","epochs_array = range(epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T21:50:47.921617Z","iopub.status.busy":"2023-11-07T21:50:47.921290Z","iopub.status.idle":"2023-11-07T21:50:48.226792Z","shell.execute_reply":"2023-11-07T21:50:48.225908Z","shell.execute_reply.started":"2023-11-07T21:50:47.921593Z"},"trusted":true},"outputs":[],"source":["# Plot Training and Test loss\n","plt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\n","# plt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\n","plt.title('Training and Test loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Infer**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T06:55:28.382136Z","iopub.status.busy":"2023-11-08T06:55:28.381375Z","iopub.status.idle":"2023-11-08T06:55:28.940632Z","shell.execute_reply":"2023-11-08T06:55:28.939793Z","shell.execute_reply.started":"2023-11-08T06:55:28.382101Z"},"trusted":true},"outputs":[],"source":["# from torch.jit import load\n","# model = UNet()\n","# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n","\n","# checkpoint = torch.load(pretrained_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T06:57:55.245656Z","iopub.status.busy":"2023-11-08T06:57:55.245294Z","iopub.status.idle":"2023-11-08T06:57:55.423820Z","shell.execute_reply":"2023-11-08T06:57:55.422815Z","shell.execute_reply.started":"2023-11-08T06:57:55.245628Z"},"trusted":true},"outputs":[],"source":["# optimizer.load_state_dict(checkpoint['optimizer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:05:00.457967Z","iopub.status.busy":"2023-11-08T07:05:00.457056Z","iopub.status.idle":"2023-11-08T07:05:00.501755Z","shell.execute_reply":"2023-11-08T07:05:00.500785Z","shell.execute_reply.started":"2023-11-08T07:05:00.457935Z"},"trusted":true},"outputs":[],"source":["# from collections import OrderedDict\n","# new_state_dict = OrderedDict()\n","# for k, v in checkpoint['model'].items():\n","#     name = k[7:] # remove `module.`\n","#     new_state_dict[name] = v\n","# # load params\n","# model.load_state_dict(new_state_dict)"]},{"cell_type":"markdown","metadata":{},"source":["**Visualize results**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:07:30.869481Z","iopub.status.busy":"2023-11-08T07:07:30.868693Z","iopub.status.idle":"2023-11-08T07:07:31.229568Z","shell.execute_reply":"2023-11-08T07:07:31.228478Z","shell.execute_reply.started":"2023-11-08T07:07:30.869445Z"},"trusted":true},"outputs":[],"source":["for i, (data, label) in enumerate(train_dataloader):\n","    img = data\n","    mask = label\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T07:07:31.234624Z","iopub.status.busy":"2023-11-08T07:07:31.234317Z","iopub.status.idle":"2023-11-08T07:08:23.681497Z","shell.execute_reply":"2023-11-08T07:08:23.680532Z","shell.execute_reply.started":"2023-11-08T07:07:31.234598Z"},"trusted":true},"outputs":[],"source":["fig, arr = plt.subplots(4, 3, figsize=(16, 12))\n","arr[0][0].set_title('Image')\n","arr[0][1].set_title('Segmentation')\n","arr[0][2].set_title('Predict')\n","\n","model.eval()\n","with torch.no_grad():\n","    predict = model(img)\n","\n","for i in range(4):\n","    arr[i][0].imshow(img[i].permute(1, 2, 0));\n","    \n","    arr[i][1].imshow(F.one_hot(mask[i]).float())\n","    \n","    arr[i][2].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())"]},{"cell_type":"markdown","metadata":{},"source":["**Create submission**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T22:07:51.294463Z","iopub.status.busy":"2023-11-07T22:07:51.294098Z","iopub.status.idle":"2023-11-07T22:07:51.299611Z","shell.execute_reply":"2023-11-07T22:07:51.298431Z","shell.execute_reply.started":"2023-11-07T22:07:51.294435Z"},"trusted":true},"outputs":[],"source":["transform = Compose([Resize((800, 1120), interpolation=InterpolationMode.BILINEAR),\n","                     PILToTensor()])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T22:07:56.929996Z","iopub.status.busy":"2023-11-07T22:07:56.929306Z","iopub.status.idle":"2023-11-07T22:07:56.936872Z","shell.execute_reply":"2023-11-07T22:07:56.935706Z","shell.execute_reply.started":"2023-11-07T22:07:56.929964Z"},"trusted":true},"outputs":[],"source":["class UNetTestDataClass(Dataset):\n","    def __init__(self, images_path, transform):\n","        super(UNetTestDataClass, self).__init__()\n","        \n","        images_list = os.listdir(images_path)\n","        images_list = [images_path+i for i in images_list]\n","        \n","        self.images_list = images_list\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        img_path = self.images_list[index]\n","        data = Image.open(img_path)\n","        h = data.size[1]\n","        w = data.size[0]\n","        data = self.transform(data) / 255        \n","        return data, img_path, h, w\n","    \n","    def __len__(self):\n","        return len(self.images_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path = '/kaggle/input/bkai-igh-neopolyp/test/test/'\n","unet_test_dataset = UNetTestDataClass(path, transform)\n","test_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, (data, path, h, w) in enumerate(test_dataloader):\n","    img = data\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, arr = plt.subplots(5, 2, figsize=(16, 12))\n","arr[0][0].set_title('Image');\n","arr[0][1].set_title('Predict');\n","\n","model.eval()\n","with torch.no_grad():\n","    predict = model(img)\n","\n","for i in range(5):\n","    arr[i][0].imshow(img[i].permute(1, 2, 0));\n","    arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.eval()\n","if not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n","    os.mkdir(\"/kaggle/working/predicted_masks\")\n","for _, (img, path, H, W) in enumerate(test_dataloader):\n","    a = path\n","    b = img\n","    h = H\n","    w = W\n","    \n","    with torch.no_grad():\n","        predicted_mask = model(b)\n","    for i in range(len(a)):\n","        image_id = a[i].split('/')[-1].split('.')[0]\n","        filename = image_id + \".png\"\n","        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n","        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rle_to_string(runs):\n","    return ' '.join(str(x) for x in runs)\n","\n","def rle_encode_one_mask(mask):\n","    pixels = mask.flatten()\n","    pixels[pixels > 0] = 255\n","    use_padding = False\n","    if pixels[0] or pixels[-1]:\n","        use_padding = True\n","        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n","        pixel_padded[1:-1] = pixels\n","        pixels = pixel_padded\n","    \n","    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n","    if use_padding:\n","        rle = rle - 1\n","    rle[1::2] = rle[1::2] - rle[:-1:2]\n","    return rle_to_string(rle)\n","\n","def mask2string(dir):\n","    ## mask --> string\n","    strings = []\n","    ids = []\n","    ws, hs = [[] for i in range(2)]\n","    for image_id in os.listdir(dir):\n","        id = image_id.split('.')[0]\n","        path = os.path.join(dir, image_id)\n","        print(path)\n","        img = cv2.imread(path)[:,:,::-1]\n","        h, w = img.shape[0], img.shape[1]\n","        for channel in range(2):\n","            ws.append(w)\n","            hs.append(h)\n","            ids.append(f'{id}_{channel}')\n","            string = rle_encode_one_mask(img[:,:,channel])\n","            strings.append(string)\n","    r = {\n","        'ids': ids,\n","        'strings': strings,\n","    }\n","    return r\n","\n","\n","MASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\n","dir = MASK_DIR_PATH\n","res = mask2string(dir)\n","df = pd.DataFrame(columns=['Id', 'Expected'])\n","df['Id'] = res['ids']\n","df['Expected'] = res['strings']\n","df.to_csv(r'output.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
